{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2e57c-3543-4a82-a7de-938304822890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third take on monthly counts, building from daily totals\n",
    "# https://chatgpt.com/share/684ad316-85dc-800c-a825-6d7e68143383 - basic setup and structure\n",
    "# https://chatgpt.com/share/684f1411-b5c8-800c-ba94-41927d889226\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae338798-5f6e-4645-87e7-9634df95c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0901e43-24ef-42e9-9932-b094cc34d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/new_KW_count.csv')\n",
    "daily_count = pd.read_csv('data/counts_by_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5651bf-fde4-4ab6-81a1-f3f08fa7d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Aggregate overall corpus to months\n",
    "# — assumes daily_count has columns ['date', 'count']\n",
    "daily_count['pub_date'] = pd.to_datetime(daily_count['pub_date'], errors='coerce')\n",
    "daily_count['month'] = daily_count['pub_date'].dt.to_period('M').dt.to_timestamp()\n",
    "monthly_total = daily_count.groupby('month')['count'].sum()\n",
    "\n",
    "# Step 2: Aggregate your subset to months (month_count)\n",
    "# — assumes df has a 'pub_date' for each topic-relevant article\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date'], errors='coerce')\n",
    "df['month'] = df['pub_date'].dt.to_period('M').dt.to_timestamp()\n",
    "month_count = df.groupby('month').size()\n",
    "\n",
    "# Step 3: Align and compute proportion\n",
    "all_months = pd.date_range(\n",
    "    start=min(monthly_total.index.min(), month_count.index.min()),\n",
    "    end=max(monthly_total.index.max(), month_count.index.max()),\n",
    "    freq='MS'\n",
    ")\n",
    "monthly_total = monthly_total.reindex(all_months, fill_value=0)\n",
    "month_count    = month_count.reindex(all_months, fill_value=0)\n",
    "\n",
    "# Build a single DataFrame\n",
    "monthly_df = pd.DataFrame({\n",
    "    'monthly_total': monthly_total,\n",
    "    'month_count':   month_count\n",
    "}, index=all_months)\n",
    "\n",
    "# Compute proportion (will be NaN where monthly_total is 0)\n",
    "monthly_df['proportion'] = monthly_df['month_count'] / monthly_df['monthly_total']\n",
    "\n",
    "# Quick look\n",
    "print(monthly_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf0191-0305-4c1f-aaa9-d81d8b06d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs (from months with zero total) by 0\n",
    "monthly_df['proportion'] = monthly_df['proportion'].fillna(0)\n",
    "\n",
    "# Rename to space_proportion\n",
    "monthly_df.rename(columns={'proportion': 'space_proportion'}, inplace=True)\n",
    "\n",
    "# Quick check\n",
    "print(monthly_df[['monthly_total', 'month_count', 'space_proportion']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acde800-4a57-40f6-90bc-8401fa979387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 0) (Optional) If you’d rather work with a generic name, rename:\n",
    "# monthly_df = monthly_df.rename(columns={'space_proportion':'proportion'})\n",
    "\n",
    "# 1) Reset index to get the month back as a column, then add month_num\n",
    "monthly_df = monthly_df.reset_index().rename(columns={'index':'month'})\n",
    "monthly_df['month_num'] = np.arange(len(monthly_df))\n",
    "\n",
    "# 2) Drop any rows where space_proportion is NaN (happens if monthly_total==0)\n",
    "data = monthly_df.dropna(subset=['space_proportion'])\n",
    "\n",
    "# 3) Build your design matrices\n",
    "X = sm.add_constant(data['month_num'])             # intercept + trend\n",
    "y = data['space_proportion']                       # your response\n",
    "\n",
    "# 4) Fit OLS\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# 5) Inspect the slope (coef of month_num), p-value, R², etc.\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6084d-96c5-4175-bc5f-b548fbbbb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210e5c7-ff7e-4ce2-be3f-5dbb7e7105fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df=pd.read_csv('data/new_astro_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c46fb-f2ec-45a0-9d44-296d14757f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the parsed‐datetime column so you only have the original strings\n",
    "events_df = events_df.drop(columns=['clean_date_dt'])\n",
    "# — or, in place:\n",
    "# events_df.drop('clean_date_dt', axis=1, inplace=True)\n",
    "\n",
    "# Quick check:\n",
    "print(events_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d926e-f09e-4a4a-b6f5-4dbee87fc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Replace all “?” with “-” so that those DDDD?DD?DD strings become DDDD-DD-DD\n",
    "events_df['clean_date'] = events_df['clean_date'].str.replace('?', '-', regex=False)\n",
    "\n",
    "# 2) First parse everything assuming ISO (YYYY-MM-DD)\n",
    "parsed_iso = pd.to_datetime(\n",
    "    events_df['clean_date'],\n",
    "    format='%Y-%m-%d',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 3) Then parse anything still NaT as slash dates (M/D/YYYY or MM/DD/YYYY)\n",
    "parsed_slash = pd.to_datetime(\n",
    "    events_df['clean_date'],\n",
    "    format='%m/%d/%Y',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 4) Combine them, preferring the ISO parse\n",
    "parsed_all = parsed_iso.fillna(parsed_slash)\n",
    "\n",
    "# 5) Report any remaining unparsed values\n",
    "still_bad = events_df.loc[parsed_all.isna(), 'clean_date'].unique()\n",
    "if len(still_bad):\n",
    "    print(\"⚠️ These still failed to parse:\", still_bad)\n",
    "else:\n",
    "    print(\"✅ All dates parsed successfully!\")\n",
    "\n",
    "# 6) Overwrite clean_date with uniform ISO strings\n",
    "events_df['clean_date'] = parsed_all.dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e35e6-235b-462e-b7d1-16e1f90609af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse event_month as a datetime (if it isn’t already)\n",
    "em = pd.to_datetime(events_df['event_month'], errors='coerce')\n",
    "\n",
    "# Show you the distribution of “day” values\n",
    "print(em.dt.day.value_counts())\n",
    "\n",
    "# And if you want to list any that aren’t the 1st:\n",
    "bad = events_df.loc[em.dt.day != 1, ['clean_date','event_month']]\n",
    "print(\"\\nThese rows are not on the 1st of the month:\\n\", bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d12a616-18f1-446f-a0e2-80a4e4ee6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Parse your clean_date into a true datetime (if you haven't already)\n",
    "events_df['clean_date_dt'] = pd.to_datetime(\n",
    "    events_df['clean_date'],\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 2) Recompute event_month as the 1st of month\n",
    "events_df['event_month'] = (\n",
    "    events_df['clean_date_dt']\n",
    "      .dt.to_period('M')       # e.g. “1907-11”\n",
    "      .dt.to_timestamp()       # → “1907-11-01 00:00:00”\n",
    ")\n",
    "\n",
    "# 3) Check that every day is now “1”\n",
    "day_counts = events_df['event_month'].dt.day.value_counts()\n",
    "print(\"Days in event_month:\", day_counts.to_dict())\n",
    "\n",
    "# 4) (Optional) List any remaining non-1 days\n",
    "bad = events_df.loc[events_df['event_month'].dt.day != 1, \n",
    "                   ['clean_date','event_month']]\n",
    "if not bad.empty:\n",
    "    print(\"Still off-1 rows:\")\n",
    "    print(bad)\n",
    "else:\n",
    "    print(\"All event_month values are now the first of the month.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b227d3-e9e0-4816-92c2-8d04fb6191a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Parse dates & drop pre-1850 rows\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date'], errors='coerce')\n",
    "df = df[df['pub_date'] >= '1850-01-01'].copy()\n",
    "\n",
    "events_df['clean_date_dt'] = pd.to_datetime(events_df['clean_date'], errors='coerce')\n",
    "events_df = events_df[events_df['clean_date_dt'] >= '1850-01-01'].copy()\n",
    "\n",
    "# --- 2) Extract publication year\n",
    "df['year']        = df['pub_date'].dt.year\n",
    "events_df['year'] = events_df['clean_date_dt'].dt.year\n",
    "\n",
    "# --- 3) Yearly article totals and topic counts\n",
    "yearly_total  = df.groupby('year').size()\n",
    "topic_cols    = [\n",
    "    'moon_count','mars_count','jupiter_count','venus_count',\n",
    "    'planet_count','comet_count','eclipse_count',\n",
    "    'mercury_count','saturn_count','neptune_count',\n",
    "    'uranus_count','satellite_count','rocket_count'\n",
    "]\n",
    "yearly_topics = df.groupby('year')[topic_cols].sum()\n",
    "\n",
    "# --- 4) Yearly proportions\n",
    "yearly_props = yearly_topics.div(yearly_total, axis=0)\n",
    "\n",
    "# --- 5) Build year-by-event_key dummies\n",
    "evt_year_counts = (\n",
    "    events_df\n",
    "      .groupby(['year','event_key'])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "evt_year_dummy = (evt_year_counts > 0).astype(int)\n",
    "\n",
    "# --- 6) Merge into one DataFrame\n",
    "yearly_analysis = (\n",
    "    yearly_props\n",
    "      .merge(evt_year_dummy, left_index=True, right_index=True, how='left')\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "# --- 7) Difference in means for each event_key\n",
    "print(\"=== Mean Δ in Proportion (Event‐Year vs Non‐Event‐Year) ===\")\n",
    "for key in evt_year_dummy.columns:\n",
    "    prop_col = next((c for c in yearly_props.columns if key.lower() in c), None)\n",
    "    if not prop_col:\n",
    "        continue\n",
    "    with_ev    = yearly_analysis.loc[yearly_analysis[key]==1, prop_col].mean()\n",
    "    without_ev = yearly_analysis.loc[yearly_analysis[key]==0, prop_col].mean()\n",
    "    print(f\"{key:15s} Δ = {with_ev - without_ev:+.4f} in {prop_col}\")\n",
    "\n",
    "# --- 8) OLS regression of each topic proportion on its event dummy\n",
    "print(\"\\n=== OLS: Proportion ~ Event‐Year Dummy ===\")\n",
    "for key in evt_year_dummy.columns:\n",
    "    prop_col = next((c for c in yearly_props.columns if key.lower() in c), None)\n",
    "    if not prop_col:\n",
    "        continue\n",
    "    y = yearly_analysis[prop_col]\n",
    "    X = sm.add_constant(yearly_analysis[key])\n",
    "    res = sm.OLS(y, X).fit()\n",
    "    print(f\"\\n--- {prop_col} ~ {key} ---\")\n",
    "    print(res.summary())\n",
    "\n",
    "# --- 9) Example plot for one event_key\n",
    "example_key = 'Mars'  # or any other event_key\n",
    "prop_col    = next(c for c in yearly_props.columns if 'mars' in c)\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(yearly_analysis.index, yearly_analysis[prop_col], marker='o', label=prop_col)\n",
    "for yr in yearly_analysis.index[yearly_analysis[example_key]==1]:\n",
    "    ax.axvline(yr, color='gray', alpha=0.3)\n",
    "ax.set_title(f\"{prop_col} with '{example_key}' event‐years highlighted\")\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a77eea-363f-448d-85fe-345df7af06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Make sure pub_date is datetime and filter 1850+\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date'], errors='coerce')\n",
    "df = df[df['pub_date'].dt.year >= 1850].copy()\n",
    "\n",
    "# 2) Compute yearly space_proportion: mean of the is_H flag\n",
    "yearly_space = (\n",
    "    df\n",
    "      .groupby(df['pub_date'].dt.year)['is_H']\n",
    "      .mean()\n",
    ")\n",
    "yearly_space.index.name = 'year'\n",
    "yearly_space.name = 'space_proportion'\n",
    "\n",
    "# 3) Identify local peaks: strictly greater than prev & next year\n",
    "peak_mask = (\n",
    "    (yearly_space > yearly_space.shift(1)) &\n",
    "    (yearly_space > yearly_space.shift(-1))\n",
    ")\n",
    "annual_peaks = yearly_space[peak_mask]\n",
    "\n",
    "# 4) Print out the peak years & values\n",
    "print(\"Annual space_proportion peaks:\")\n",
    "print(annual_peaks)\n",
    "\n",
    "# 5) (Optional) Plot the full series with peaks highlighted\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(yearly_space.index, yearly_space.values, marker='o', label='space_proportion')\n",
    "ax.scatter(annual_peaks.index, annual_peaks.values, color='red', zorder=5, label='peaks')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Space Proportion')\n",
    "ax.set_title('Yearly Space Proportion with Local Peaks')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd89abe-d8c2-47c4-971c-5ccb486c941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Build a mapping from each event_key → its yearly proportion column\n",
    "mapping = {}\n",
    "for key in evt_year_dummy.columns:\n",
    "    matches = [c for c in yearly_props.columns if key.lower() in c]\n",
    "    if matches:\n",
    "        mapping[key] = matches[0]\n",
    "\n",
    "# 2) Scan each topic for local‐max years with no event\n",
    "orphan_list = []\n",
    "for key, prop_col in mapping.items():\n",
    "    s = yearly_props[prop_col]\n",
    "    # strict local peaks: > previous year & > next year\n",
    "    peak_mask = (s > s.shift(1)) & (s > s.shift(-1))\n",
    "    peaks = s[peak_mask].dropna()\n",
    "\n",
    "    # align the event‐year dummy to those peak years\n",
    "    ev = evt_year_dummy[key].reindex(peaks.index, fill_value=0)\n",
    "\n",
    "    # keep only the peaks where ev == 0 (no event that year)\n",
    "    orphan_years = peaks[ev == 0]\n",
    "    for year, val in orphan_years.items():\n",
    "        orphan_list.append({\n",
    "            'event_key':  key,\n",
    "            'peak_year':  year,\n",
    "            'proportion': val\n",
    "        })\n",
    "\n",
    "# 3) Build a DataFrame and sort\n",
    "orphan_peaks_annual = (\n",
    "    pd.DataFrame(orphan_list)\n",
    "      .sort_values(['event_key', 'peak_year'])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 4) Inspect\n",
    "print(orphan_peaks_annual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341dd998-dbba-4ff6-903b-b604d3a0d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = orphan_peaks_annual['proportion'].quantile(0.75)\n",
    "strong_orphans = orphan_peaks_annual[\n",
    "    orphan_peaks_annual['proportion'] >= threshold\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58318280-6fdb-4904-a2ef-513e62bf94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for key in evt_year_dummy.columns:\n",
    "    # find the matching proportion column (e.g. 'Mars' → 'mars_count')\n",
    "    prop_col = next((c for c in yearly_props.columns if key.lower() in c), None)\n",
    "    if not prop_col:\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    years = yearly_analysis.index\n",
    "\n",
    "    # 1) Plot the yearly proportion\n",
    "    ax.plot(years, yearly_analysis[prop_col], marker='o', label=prop_col)\n",
    "\n",
    "    # 2) Vertical lines for event‐years\n",
    "    evt_years = yearly_analysis.index[yearly_analysis[key] == 1]\n",
    "    for yr in evt_years:\n",
    "        ax.axvline(yr, color='gray', alpha=0.3)\n",
    "\n",
    "    # 3) Overlay orphan peaks for this key\n",
    "    peaks = strong_orphans[strong_orphans['event_key'] == key]\n",
    "    if not peaks.empty:\n",
    "        ax.scatter(peaks['peak_year'], peaks['proportion'],\n",
    "                   color='red', marker='X', s=100, label='Orphan peaks')\n",
    "        # Add text labels at each point\n",
    "        for _, row in peaks.iterrows():\n",
    "            ax.text(row['peak_year'], row['proportion'],\n",
    "                    str(int(row['peak_year'])),\n",
    "                    color='red', fontsize=9,\n",
    "                    ha='left', va='bottom')\n",
    "\n",
    "    # 4) Labels & legend\n",
    "    ax.set_title(f\"{prop_col} with '{key}' Events & Orphan Peaks\")\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Proportion')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae33cb-9fff-4667-b220-10746306de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 0) Make sure your events_df has a datetime column; for example:\n",
    "# if you have `event_month` as a string “YYYY-MM-DD”, parse it:\n",
    "events_df['event_month_dt'] = pd.to_datetime(\n",
    "    events_df['event_month'], \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 1) Reduce to the first‐of‐month timestamp\n",
    "events_df['month'] = (\n",
    "    events_df['event_month_dt']\n",
    "      .dt.to_period('M')\n",
    "      .dt.to_timestamp()\n",
    ")\n",
    "\n",
    "# 2) Pivot into counts of events per month × event_key\n",
    "evt_counts = (\n",
    "    events_df\n",
    "      .groupby(['month','event_key'])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# 3) Turn counts into a boolean dummy (1 if ≥1 event, else 0)\n",
    "evt_dummy = (evt_counts > 0).astype(int)\n",
    "\n",
    "# Now you can rerun your orphan‐peak code using this evt_dummy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c10a24-c5fa-4dca-bdbb-f4b3a4483183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Make sure your article dates are datetimes and extract month\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date'], errors='coerce')\n",
    "df['month']    = df['pub_date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# 2) Define the columns you want to turn into proportions\n",
    "topic_cols = [\n",
    "    'moon_count','mars_count','jupiter_count','venus_count',\n",
    "    'planet_count','comet_count','eclipse_count',\n",
    "    'mercury_count','saturn_count','neptune_count',\n",
    "    'uranus_count','satellite_count','rocket_count'\n",
    "]\n",
    "\n",
    "# 3) Compute total articles per month\n",
    "monthly_total = df.groupby('month').size()\n",
    "\n",
    "# 4) Sum each topic’s raw counts by month\n",
    "monthly_topics = df.groupby('month')[topic_cols].sum()\n",
    "\n",
    "# 5) (Optional) Align to a complete monthly index, filling gaps with zeros\n",
    "all_months = pd.date_range(\n",
    "    start=monthly_total.index.min(),\n",
    "    end=monthly_total.index.max(),\n",
    "    freq='MS'\n",
    ")\n",
    "monthly_total  = monthly_total.reindex(all_months, fill_value=0)\n",
    "monthly_topics = monthly_topics.reindex(all_months, fill_value=0)\n",
    "\n",
    "# 6) Divide to get proportions\n",
    "monthly_props = monthly_topics.div(monthly_total, axis=0)\n",
    "\n",
    "# -- Now monthly_props is ready for your orphan‐peak or any other analyses. --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba223a13-98c5-40f4-90ad-af557f02c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# — assume monthly_props and evt_dummy are already defined —\n",
    "\n",
    "# 1) Collect monthly orphan peaks\n",
    "monthly_orphans = []\n",
    "# Build a Series that’s True in months with ANY event\n",
    "any_event_months = (evt_dummy.sum(axis=1) > 0)\n",
    "\n",
    "for prop_col in monthly_props.columns:\n",
    "    s = monthly_props[prop_col]\n",
    "    # local‐maxima: strictly above both neighbors\n",
    "    peaks = s[(s > s.shift(1)) & (s > s.shift(-1))].dropna()\n",
    "    # pick only those months where NO event of any kind occurred\n",
    "    orphan_months = peaks[~any_event_months.reindex(peaks.index, fill_value=False)]\n",
    "    for month, val in orphan_months.items():\n",
    "        monthly_orphans.append({\n",
    "            'topic':      prop_col,\n",
    "            'peak_month': month,\n",
    "            'proportion': val\n",
    "        })\n",
    "\n",
    "# 2) Build the DataFrame\n",
    "monthly_orphans_df = pd.DataFrame(monthly_orphans)\n",
    "\n",
    "# 3) Sanity check & display\n",
    "if monthly_orphans_df.empty:\n",
    "    print(\"🔍 No monthly orphan peaks found — every local peak coincides with at least one event.\")\n",
    "    # (Optional) List out all local peaks, regardless of events:\n",
    "    print(\"\\nAll local peaks by topic (for reference):\")\n",
    "    for prop_col in monthly_props.columns:\n",
    "        s = monthly_props[prop_col]\n",
    "        peaks = s[(s > s.shift(1)) & (s > s.shift(-1))].dropna()\n",
    "        print(f\" • {prop_col}: {peaks.index.tolist()}\")\n",
    "else:\n",
    "    monthly_orphans_df = (\n",
    "        monthly_orphans_df\n",
    "          .sort_values(['topic','peak_month'])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    print(monthly_orphans_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688e64a-e9a9-46a9-96b5-1220f7442a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Group and summarize\n",
    "summary = (\n",
    "    monthly_orphans_df\n",
    "      .groupby('topic')\n",
    "      .agg(\n",
    "          orphan_count     = ('peak_month', 'count'),\n",
    "          max_proportion   = ('proportion', 'max')\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(by='max_proportion', ascending=False)\n",
    ")\n",
    "\n",
    "# 2) Show the result\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
