{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grfaith/May25/blob/main/25May13_lookup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "660E_6iWszLZ"
      },
      "outputs": [],
      "source": [
        "# ── 0. one-time setup ───────────────────────────────────────────────\n",
        "from google.colab import drive\n",
        "import os, zipfile, datetime as dt\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)   # one prompt, then silent\n",
        "OUT_DIR = '/content/drive/MyDrive/AmStories_kw_hits'\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"AS_kw_May25_string_word.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ZMgC6UJ3BLN0oGmtUVPLJ8We45WttGLs\n",
        "\"\"\"\n",
        "\n",
        "###Installs\n",
        "\n",
        "!pip install datasets\n",
        "!pip install ipympl\n",
        "!pip install --upgrade datasets\n",
        "\n",
        "#Imports\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import tqdm as tq\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# kw_distance = 150\n",
        "\n",
        "### Cell for loading files from local drive\n",
        "kw_file = files.upload()\n",
        "\n",
        "# Specify custom column names\n",
        "column_names = [\"kword\", \"kwyear\", \"kwtype\"]\n",
        "\n",
        "# Read the uploaded CSV file into a DataFrame with custom column names\n",
        "for fn in kw_file.keys():\n",
        "    kw_df = pd.read_csv(fn, names=column_names, header=None)\n",
        "\n",
        "# Display information about the uploaded file\n",
        "for fn in kw_file.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "          name=fn, length=len(kw_file[fn])))\n",
        "\n",
        "# Defining function to load dataset\n",
        "\n",
        "def load_text_dataset(dataset_year_str):\n",
        "    \"\"\"\n",
        "    This function pulls a dataset of a specific year from the HuggingFace Hub.\n",
        "\n",
        "    Parameters:\n",
        "        dataset_year (int): The year of the dataset to be pulled..\n",
        "\n",
        "    Returns:\n",
        "        dataset_article_level: dataset for appropriate year\n",
        "    \"\"\"\n",
        "    # Download data for the dataset year at the associated article level (Default)\n",
        "    # dataset = load_dataset(\"dell-research-harvard/AmericanStories\", \"subset_years\", year_list=[dataset_year])\n",
        "\n",
        "    # now let's load our data, we have to specify the huggingface location of our\n",
        "    # data, the fact that we want to have a subset of years, and our desired years\n",
        "    dataset_article_level=load_dataset(\"dell-research-harvard/AmericanStories\",\n",
        "                                      \"subset_years\",\n",
        "                                       year_list=[dataset_year_str],\n",
        "                                       trust_remote_code=True\n",
        "                                       )\n",
        "\n",
        "    return dataset_article_level\n",
        "\n",
        "### Function to filter kw data in use based on years of discovery in kw file.\n",
        "\n",
        "def get_kw(dataset_year_str):\n",
        "    \"\"\"\n",
        "    This function loads a CSV file to create a DataFrame and filters out keywords where the second column is less than 1774\n",
        "    Parameters:\n",
        "        kw_file loaded from prompt above\n",
        "    Returns:\n",
        "        pandas.DataFrame: The filtered Data\n",
        "    \"\"\"\n",
        "    # Convert dataset_year to integer\n",
        "    dataset_year = int(dataset_year_str)\n",
        "\n",
        "    # Filter the rows based on the condition\n",
        "    kw_df_filter = kw_df[kw_df['kwyear'] <= dataset_year]\n",
        "\n",
        "    # print(kw_df_filter)\n",
        "\n",
        "    return kw_df_filter\n",
        "\n",
        "def process_kw(dataset_year_str, kw_df_filter, dataset_article_level):\n",
        "    \"\"\"\n",
        "    This function processes words in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        kw_df_filter (pandas.DataFrame): The DataFrame containing keywords\n",
        "        dataset_article (DatasetDict): A dictionary-like object containing datasets for different years.\n",
        "\n",
        "    Returns:\n",
        "        dataset_year_hits (DataFrame or None): A DataFrame containing results if found, or None if no results were found.\n",
        "    \"\"\"\n",
        "    print (\"Searching within \", dataset_year_str)\n",
        "\n",
        "    # Creating an empty dataframe\n",
        "    current_year_df = pd.DataFrame()\n",
        "\n",
        "    for index, row in kw_df_filter.iterrows():\n",
        "        explore_kw = row.iloc[0]\n",
        "        kw_type = row.iloc[2]\n",
        "        # print(explore_kw, kw_type)\n",
        "        result_df = kw_search(dataset_article_level, dataset_year_str, explore_kw, kw_type)\n",
        "        # Concatenate the single search result onto the results DataFrame\n",
        "        current_year_df = pd.concat([current_year_df, result_df], ignore_index=True)\n",
        "\n",
        "    return current_year_df\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def kw_search(dataset_article, dataset_year, explor_kw, kw_type):\n",
        "    \"\"\"\n",
        "    This function searches through the dataset for matching articles containing the given keyword.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access the dataset for the specific year\n",
        "        year_dataset = dataset_article[dataset_year]\n",
        "\n",
        "        # Check the structure of the year dataset\n",
        "        # print(f\"Dataset structure for year {dataset_year}: {year_dataset.column_names}\")\n",
        "\n",
        "        # Access the 'article' column containing the text\n",
        "        articles = year_dataset['article']\n",
        "\n",
        "        # Create an empty list to store matching articles\n",
        "        articles_containing_kw = []\n",
        "\n",
        "        for article_n, article_text in enumerate(articles):\n",
        "            article_text = article_text.lower()\n",
        "\n",
        "            # Determine the search pattern based on kw_type\n",
        "            if kw_type == \"string\":\n",
        "                # Define the pattern to search for the value of 'explor_kw' within words (partial match)\n",
        "                pattern_kw = re.compile(r'\\b\\w*' + re.escape(explor_kw) + r'\\w*\\b', flags=re.IGNORECASE)\n",
        "            elif kw_type == \"word\":\n",
        "                # Define the pattern to search for the exact value of 'explor_kw' as a whole word (exact match)\n",
        "                pattern_kw = re.compile(r'\\b' + re.escape(explor_kw) + r'\\b', flags=re.IGNORECASE)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid kw_type. Must be either 'string' or 'word'.\")\n",
        "\n",
        "            # Find all occurrences of the keyword\n",
        "            kw_matches = pattern_kw.findall(article_text)\n",
        "            kw_count = len(kw_matches)\n",
        "\n",
        "            # If the keyword is found, add the article info and keyword count to the results\n",
        "            if kw_count > 0:\n",
        "                # print(f\"Keyword '{explor_kw}' found in article {article_n} with {kw_count} occurrences.\")\n",
        "\n",
        "                # Check if 'article_id' is available\n",
        "                if \"article_id\" in year_dataset[article_n]:\n",
        "                    article_id = year_dataset[article_n][\"article_id\"]\n",
        "                else:\n",
        "                    # print(f\"No 'article_id' found for article {article_n}. Skipping...\")\n",
        "                    continue\n",
        "\n",
        "                articles_containing_kw.append({\n",
        "                    'row_number': article_n,\n",
        "                    'article_ID': article_id,\n",
        "                    'keyword_hit': explor_kw,\n",
        "                    'keyword_count': kw_count,\n",
        "                })\n",
        "\n",
        "        # Check if any articles were found\n",
        "        if not articles_containing_kw:\n",
        "            # print(f\"No articles found containing the keyword '{explor_kw}' for year {dataset_year}.\")\n",
        "            # Return an empty DataFrame with the expected column names\n",
        "            return pd.DataFrame(columns=['row_number', 'article_ID', 'keyword_hit', 'keyword_count'])\n",
        "\n",
        "        # Convert the list of dictionaries to a DataFrame with the required columns\n",
        "        df_of_articles_containing_kw = pd.DataFrame(articles_containing_kw)\n",
        "\n",
        "        # Check if required columns are present before subsetting\n",
        "        if set(['row_number', 'article_ID', 'keyword_hit', 'keyword_count']).issubset(df_of_articles_containing_kw.columns):\n",
        "            df_of_articles_containing_kw = df_of_articles_containing_kw[['row_number', 'article_ID', 'keyword_hit', 'keyword_count']]\n",
        "        else:\n",
        "            print(\"Required columns are missing in the DataFrame.\")\n",
        "\n",
        "        return df_of_articles_containing_kw\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"KeyError: {e}. Please check if the dataset and article structure is correct.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\"\"\"# *BREAK*\"\"\"\n",
        "\n",
        "# Full AmStories extends back to 1774 (I think). Previous searches have returned many  hits\n",
        "full_start_year = 1844  # Inclusive\n",
        "full_end_year = 1845    # Exclusive, so 1845 is not included\n",
        "chunk_size = 3\n",
        "\n",
        "# ── 2.  main loop, but drop files.download() ────────────────────────\n",
        "for chunk_start in range(full_start_year, full_end_year, chunk_size):\n",
        "    chunk_end = min(chunk_start + chunk_size, full_end_year)\n",
        "    for loop_year in range(chunk_start, chunk_end):\n",
        "        dataset_year_str = str(loop_year)\n",
        "        try:\n",
        "            dataset_article_level = load_text_dataset(dataset_year_str)\n",
        "            kw_df_filter        = get_kw(loop_year)\n",
        "            year_search_result  = process_kw(dataset_year_str,\n",
        "                                             kw_df_filter,\n",
        "                                             dataset_article_level)\n",
        "\n",
        "            # save directly to Drive\n",
        "            out_path = f'{OUT_DIR}/AS_Main_KW_Hits_May25_SW_{loop_year}.csv'\n",
        "            year_search_result.to_csv(out_path, index=False)\n",
        "            print(f'✅  {out_path} written ({len(year_search_result):,} rows)')\n",
        "\n",
        "        except ValueError:\n",
        "            print(f'Dataset empty for {dataset_year_str}; skipping.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMRXwByBluH37pNPoFRy1n4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}